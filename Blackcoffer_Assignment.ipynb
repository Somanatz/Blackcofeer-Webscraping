{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0604ea-0682-4c2d-b920-5f44abebe3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for positive words file: ascii\n",
      "Encoding for negative words file: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "\n",
    "positive_words_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\positive-words.txt\"\n",
    "negative_words_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\negative-words.txt\"\n",
    "\n",
    "positive_encoding = detect_encoding(positive_words_path)\n",
    "negative_encoding = detect_encoding(negative_words_path)\n",
    "\n",
    "print(f\"Encoding for positive words file: {positive_encoding}\")\n",
    "print(f\"Encoding for negative words file: {negative_encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae793264-1881-4879-af11-75e41bf68f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import textstat\n",
    "import chardet\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "# Function to extract article text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Attempt to extract main content\n",
    "        main_content = None\n",
    "        for tag in ['article', 'div', 'main']:\n",
    "            main_content = soup.find(tag, {'class': 'main-content'})\n",
    "            if main_content:\n",
    "                break\n",
    "\n",
    "        # Fallback to extracting all paragraphs if main content not found\n",
    "        if not main_content:\n",
    "            paragraphs = soup.find_all('p')\n",
    "        else:\n",
    "            paragraphs = main_content.find_all('p')\n",
    "        \n",
    "        title = soup.find('h1').get_text() if soup.find('h1') else 'No Title'\n",
    "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
    "        return title + ' ' + article_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to detect encoding\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "\n",
    "# Load the input Excel file\n",
    "input_file_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\input.xlsx\"\n",
    "input_df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Detect encoding of positive and negative words files\n",
    "positive_words_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\positive-words.txt\"\n",
    "negative_words_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\negative-words.txt\"\n",
    "\n",
    "positive_encoding = detect_encoding(positive_words_path)\n",
    "negative_encoding = detect_encoding(negative_words_path)\n",
    "\n",
    "# Load positive and negative words with detected encoding\n",
    "positive_words = set(pd.read_csv(positive_words_path, header=None, encoding='ascii')[0].str.lower())\n",
    "negative_words = set(pd.read_csv(negative_words_path, header=None, encoding='ISO-8859-1')[0].str.lower())\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Loop through each URL and perform extraction and analysis\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    try:\n",
    "        # Extract the article text\n",
    "        article_text = extract_text(url)\n",
    "        if article_text is None:\n",
    "            raise ValueError(f\"Failed to retrieve content from {url}\")\n",
    "        \n",
    "        # Save the article text to a file\n",
    "        with open(f'{url_id}.txt', 'w', encoding='utf-8', errors='ignore') as file:\n",
    "            file.write(article_text)\n",
    "        \n",
    "        # Clean and tokenize the text\n",
    "        tokens = clean_text(article_text)\n",
    "        word_count = len(tokens)\n",
    "        sentence_count = len(sent_tokenize(article_text))\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "        negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
    "        \n",
    "        # Readability metrics\n",
    "        avg_sentence_length = word_count / sentence_count\n",
    "        complex_words = [word for word in tokens if textstat.syllable_count(word) > 2]\n",
    "        complex_word_count = len(complex_words)\n",
    "        percentage_of_complex_words = complex_word_count / word_count\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "        \n",
    "        # Additional metrics\n",
    "        syllable_count_per_word = sum(textstat.syllable_count(word) for word in tokens) / word_count\n",
    "        personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', article_text, re.I))\n",
    "        avg_word_length = sum(len(word) for word in tokens) / word_count\n",
    "        \n",
    "        # Store the results\n",
    "        results.append([\n",
    "            url_id, url, positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "            avg_sentence_length, percentage_of_complex_words, fog_index, complex_word_count,\n",
    "            word_count, syllable_count_per_word, personal_pronouns, avg_word_length\n",
    "        ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL_ID {url_id}: {e}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "columns = [\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "]\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_excel_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\Structure.xlsx\"\n",
    "results_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_csv_path = \"C:\\\\Users\\\\harik\\\\staragile assessments\\\\Structure.csv\"\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Instructions on how to run the script\n",
    "instructions = \"\"\"\n",
    "1. Ensure you have the required Python libraries installed:\n",
    "   - requests\n",
    "   - beautifulsoup4\n",
    "   - pandas\n",
    "   - nltk\n",
    "   - textstat\n",
    "   - chardet\n",
    "\n",
    "2. Place the script in the same directory as the 'input.xlsx' file.\n",
    "\n",
    "3. Ensure 'positive-words.txt' and 'negative-words.txt' files are available at the specified paths.\n",
    "\n",
    "4. Run the script using the command:\n",
    "   python script.py\n",
    "\n",
    "5. The script will generate text files for each URL_ID and an output Excel and CSV file with the analysis results.\n",
    "\"\"\"\n",
    "\n",
    "with open('instructions.txt', 'w', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write(instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc6cb6-9792-4cfa-a933-70dd4ba64410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
